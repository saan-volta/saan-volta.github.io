<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Saan</title>
        <link>//localhost:1313/</link>
        <description>Recent content on Saan</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Tue, 30 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>How to Measure</title>
        <link>//localhost:1313/post/how_to_measure/</link>
        <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/post/how_to_measure/</guid>
        <description>&lt;h3 id=&#34;0-what-is-this&#34;&gt;0. What is this?
&lt;/h3&gt;&lt;p&gt;This is intended to be an in-detail walkthrough of how measure-theoretic probability works under the hood. When I took my first course on this topic, I found that there was not a lot of material that actually connected the measure theory &amp;ldquo;machine code&amp;rdquo; to probability problems. I&amp;rsquo;ve put together some explanations and examples that would&amp;rsquo;ve been useful to me when I first learned about it. This assumes knowledge of (very) basic measure theory &amp;amp; probability.&lt;/p&gt;
&lt;h3 id=&#34;1-heres-what-you-have-to-know&#34;&gt;1. Here&amp;rsquo;s what you have to know
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;[!info] Definition: Measure space
$(\Omega,\mathcal{F},\mu)$ is a measure (probability) space, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$ is a set (of outcomes);&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$ is a $\sigma$-algebra on $\Omega$, consisting of subsets of $\Omega$ and satisfying the following:
&lt;ol&gt;
&lt;li&gt;$\varnothing,\Omega\in\mathcal{F}$;&lt;/li&gt;
&lt;li&gt;$A\in\mathcal{F}\Rightarrow A^{c}\in\mathcal{F}$ (stable under complement);&lt;/li&gt;
&lt;li&gt;$\set{A_{n}}\in\mathcal{F}\Rightarrow \bigcup_{n}A_{n}\in\mathcal{F}$ (stable under countable union);&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In probability, this is the event space.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\mu:\mathcal{F}\rightarrow [0,\infty]$ is the measure function
&lt;ul&gt;
&lt;li&gt;$\mu$ satisfies $\sigma$-additivity, i.e.: $\mu(\bigsqcup_{n}A_{n})=\sum_{n}\mu(A_{n})$;&lt;/li&gt;
&lt;li&gt;$\mu(\varnothing)=0$;&lt;/li&gt;
&lt;li&gt;In probability, the measure is written as $P$, has codomain $[0,1]$, and $P(\Omega)=1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;Comments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{F}$ is stable by (countable) intersection;&lt;/li&gt;
&lt;li&gt;$P(\varnothing)=0$ and $P(\Omega)=1$ mean that &lt;em&gt;something&lt;/em&gt; must happen;&lt;/li&gt;
&lt;li&gt;The trivial (coarsest) $\sigma$-algebra over $\Omega$ is $\set{\Omega,\varnothing}$, and the fullest (finest) is $\mathcal{P}(\Omega)$ (if $\Omega$ is countable).&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;[!info] Definition: Random variable
A (real-valued) random variable in a probability space $(\Omega,\mathcal{F},P)$ is a function $X:\Omega\rightarrow \mathbb{R}$ that is measurable, i.e., satisfies:
&lt;/p&gt;
$$\forall B\in\mathcal{B}(\mathbb{R})\quad X^{-1}(B)\in\mathcal{F}.$$&lt;p&gt;
where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra of the real line.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If you have no measure theory background, $\mathcal{B}(\mathbb{R})$ might be hard to conceptualize. It is not the powerset of $\mathbb{R}$, but it may be useful to think of it that way. The subsets of $\mathbb{R}$ which do not fall in it require nontrivial and fairly contrived constructions that will never come up in a probability application.&lt;/p&gt;
&lt;p&gt;Here are some useful identities/definitions about expectation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!note]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E[X\mid Y]=E[X\mid\sigma(Y)]$ (this is just a shorthand)&lt;/li&gt;
&lt;li&gt;$E[X]=E[E[X\mid \mathcal{A}]]$ (law of total expectation)&lt;/li&gt;
&lt;li&gt;$E[\mathbb{1}_{A}X]=E[\mathbb{1}_{A}E[X\mid\mathcal{A}]]\quad \forall A\in\mathcal{A}\quad$ (def. of conditional expectation)&lt;/li&gt;
&lt;li&gt;$E[aX+bY]=aE[X]+bE[Y]$ (linearity of expectation)&lt;/li&gt;
&lt;li&gt;$E[X\mid \mathcal{A}]=X$ when $X$ is $\mathcal{A}$-measurable and $E[X]$ when $X,\mathcal{A}$  indep.&lt;/li&gt;
&lt;li&gt;$E[XY\mid\mathcal{A}]=XE[Y\mid\mathcal{A}]$ when $X$ is $\mathcal{A}$-measurable&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;2-the-conditioner&#34;&gt;2. The conditioner
&lt;/h3&gt;&lt;p&gt;Let $(\Omega,\mathcal{F},P)$ be a probability space and let $A\in\mathcal{F}$ be an event. In this segment, we will examine what it means to condition $P(A\mid \cdot)$ on a $\sigma$-algebra.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!note] Note
I want to emphasize that when we write $P(A)$, what we mean mathematically is $E[\mathbb{1}_{A}]=\int_{\Omega}\mathbb{1}_{A}dP$, and more explicitly $\int_{\Omega}\mathbb{1}_{A}(\omega)dP(\omega)$, where $\mathbb{1}_{A}(\cdot)$ is in fact a random variable.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s look at the easiest case: suppose $B\in\mathcal{F}$ is another event, then $P(A\mid B)$ is just a number. Here, we simply condition on the realization that $B$ occurs.&lt;/p&gt;
&lt;p&gt;What if we don&amp;rsquo;t know whether it does? In such a case, we would condition on $\sigma(B)$, the $\sigma$-algebra generated by this event. The interesting thing is that $P(A\mid\sigma(B))$ is in fact a random variable (a measurable function of $\omega$). Intuitively, $\sigma(B)$ gives us some information about $A$, but the information is random and dependent on where $\omega$ lands.
Since $\sigma(B)=\set{\varnothing,B,B^{c},\Omega}$ has two nontrivial possibilities, the value of $P(A\mid\sigma(B))(\omega)$ depends on whether $\omega\in B$ or $\in B^{c}$:
&lt;/p&gt;
$$P(A\mid\sigma(B))=\mathbb{1}_{B}P(A|B)+\mathbb{1}_{B^{c}}P(A|B^{c}).$$&lt;p&gt;
This works out nicely because $\set{B,B^{c}}$ is a partition of the outcome space.
We can easily extend this &amp;ndash; let $\set{B_{n}}$ be a partition, i.e., a collection of disjoint subsets of $\Omega$ that add up to the whole set. Then:
&lt;/p&gt;
$$P(A\mid\sigma\set{B_{n}})=\sum\limits_{n}\mathbb{1}_{B_{n}}P(A\mid B_{n}).$$&lt;p&gt;
We extend this further to compute the expectation of a general random variable:
&lt;/p&gt;
$$E[X\mid\sigma\set{B_{n}}]=\sum\limits_{n}\mathbb{1}_{B_{n}} E[X\mid B_{n}]=\sum\limits_{n}\mathbb{1}_{B_{n}} \frac{1}{P(B_{n})}\int_{B_{n}}XdP.$$&lt;h3 id=&#34;3-measurability&#34;&gt;3. Measurability
&lt;/h3&gt;&lt;p&gt;Let $\mathcal{H}\subset \mathcal{F}$ and $A\in \mathcal{F}$. The definition of measurable functions tells us that $A$ being $\mathcal{H}$-measurable means that $\forall B\in\mathcal{B}(\mathbb{R}), \mathbb{1}_{A}^{-1}(B)\in \mathcal{H}$. There are four cases, dictated by whether $0$ and $1$ are in $B$. If they both are or aren&amp;rsquo;t, we get that the preimage is respectively $\Omega$ or $\varnothing$, which is trivially true as $\mathcal{H}$ contains them by definition. If only $1\in B$, then the preimage is $\set{\omega\in A}$, and if $0\in B$ it is $\set{\omega\notin A}$. Note that it is sufficient for one of these to be in $\mathcal{H}$, since the other will be contained automatically as complement. Since $\sigma(A)\subseteq \mathcal{H}$, $A$ is thus completely determinable. So we have:
&lt;/p&gt;
$$A\in\mathcal{H}\Leftrightarrow \mathbb{1}_{A}(\cdot)\; \mathcal{H}\text{-measurable }\Leftrightarrow P(A\mid\mathcal{H})=E[\mathbb{1}_{A}\mid\mathcal{H}]=\mathbb{1}_{A}\in \set{0,1}\text{ a.s. }$$&lt;p&gt;Let&amp;rsquo;s now consider that $0&lt;P(A\mid\mathcal{H})&lt;1 \text{ a.s.}$ This is a fairly strong statement about the relationship of $A$ and $\mathcal{H}$; intuitively it means that the occurrence of $A$ cannot be deterministically established by any event in $\mathcal{H}$:
&lt;/p&gt;
$$\forall B\in\mathcal{H}\text{ with }P(B)&gt;0, P(A\cap B)\neq 0,1.$$&lt;p&gt;
Define $\mathcal{H&#39;}=\mathcal{H}\lor \sigma(A)$, the $\sigma$-algebra generated by including $A$ into $\mathcal{H}$. What does a $A&#39;\in\mathcal{H&#39;}$ look like? Here, $\set{A,A^{c}}$ partitions the outcomes in two, so $A&#39;$ will consists of the union of some set $B\in\mathcal{H}$ intersecting with one part, and some other set $C$ intersecting with the other: $A&#39;=(B\cap A)\cup (C\cap A^{c})\text{ with }B,C\in\mathcal{H}$. We can view the extension by $\sigma(A)$ as adding one bit of information into the event space.&lt;/p&gt;
&lt;p&gt;![[Pasted image 20251225190318.png]]
$A&#39;$ aligns exactly with $B$ in $A$, and with $C$ in $A^{c}$.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a result that simplifies the form for indeterminate events in $\mathcal{H}&#39;$:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!tip] Proposition 1
Suppose $P(A\mid\mathcal{H})\in(0,1)\text{ a.s.}$, $\mathcal{H&#39;}=\mathcal{H}\lor\sigma(A)$, and $A&#39;\in\mathcal{H&#39;}$. Then:
&lt;/p&gt;
$$P(A&#39;\mid\mathcal{H})\in (0,1)\text{ a.s.}\iff \exists B\in\mathcal{H}\text{ s.t. }A&#39;=(B\cap  A)\cup  (B^{c}\cap A^{c}).$$&lt;/blockquote&gt;
&lt;p&gt;This gives us a characterization that an event in $\mathcal{H&#39;}$ has $0&lt;P(A&#39;\mid\mathcal{H})&lt;1$, meaning that its occurrence can never be determined from an event in $\mathcal{H}$, exactly IFF you can find $B$ and $C$ to represent $A&#39;$ with the property $B^{c}=C$. Since $B\mapsto (B\cap  A)\cup  (B^{c}\cap A^{c})$ is a bijective map, there are exactly $|\mathcal{H}|$ such indeterminate events.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!proof]-
$(\Rightarrow )$ Given $0&lt;P(A&#39;\mid\mathcal{H})&lt;1\text{ a.s.}$, take $B,C\in\mathcal{H}\text{ s.t. }A&#39;=(B\cap A)\cup (C\cap A^{c})$. Then:
&lt;/p&gt;
$$\begin{align*}
P(A&#39;\mid\mathcal{H})
&amp;= P((B\cap A)\sqcup (C\cap A^{c})\mid\mathcal{H})\\
&amp;= E[\mathbb{1}_{A}\mathbb{1}_B\mid\mathcal{H}]+E[\mathbb{1}_{C}\mathbb{1}_{A^{c}}\mid\mathcal{H}]\\
&amp;= \mathbb{1}_{B}P(A\mid\mathcal{H})+\mathbb{1}_{C}P(A^{c}\mid\mathcal{H})\\
&amp;\in (0,1).
\end{align*}$$&lt;p&gt;
The $&gt;0$ implies that $B\cup C=\Omega$, and $&lt;1$ gives us $B\cap C=\varnothing$, thus they are complements.
$(\Leftarrow )$ is easy.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;4-change-of-measure-and-independence&#34;&gt;4. Change of measure and independence
&lt;/h3&gt;&lt;p&gt;Let&amp;rsquo;s examine another angle of the property $0&lt;P(A\mid\mathcal{H})&lt;1 \text{ a.s.}$ Since $\mathcal{H}$ is an arbitrary sub-$\sigma$-algebra (not necessarily formed by a nice partition), we can&amp;rsquo;t really tell how the probability is changed by the conditioning.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!info] Definition: Equivalence of measures
Let $\mu,\nu:\mathcal{F}\rightarrow \mathbb{R}_{\geq0}$. They are considered equivalent if their null sets are the same, i.e., $\forall F\in\mathcal{F},\; \mu(F)=0\iff \nu(F)=0.$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We can construct a measure $Q:\mathcal{F}\rightarrow [0,1]$ that is structurally equivalent to $P$, but under which $A$ is independent of $\mathcal{H}$. For this, we will &amp;ldquo;re-weigh&amp;rdquo; the measures for events in a way that gives us $Q(A\mid\mathcal{H})=Q(A)=\alpha$ for some $\alpha\in(0,1)$ of our choosing.&lt;/p&gt;
&lt;p&gt;Consider the function
&lt;/p&gt;
$$f_\alpha:\Omega\rightarrow [0,1]\text{ with }f_{\alpha}= \left(\alpha \frac{\mathbb{1}_{A}}{P(A\mid\mathcal{H})} + \left(1-\alpha\right)\frac{\mathbb{1}_{A^{c}}}{P(A^{c}\mid\mathcal{H})}\right).$$&lt;p&gt;
What does this function do? When $\omega$ lands in $A$, we scale the it by the fixed constant $\alpha$ and normalize by $P(A\mid\mathcal{H})$, which is just a &amp;ldquo;fixed&amp;rdquo; function; when it lands outside we likewise scale and normalize by the complements. With this $f$, we can construct our weighted measure of interest:
&lt;/p&gt;
$$\forall B\in\mathcal{F},\quad Q_\alpha(B):=\int_{B}fdP=E_{P}\left[\alpha \frac{\mathbb{1}_{A\cap B}}{P(A\mid\mathcal{H})} + \left(1-\alpha\right)\frac{\mathbb{1}_{A^{c}\cap B}}{P(A^{c}\mid\mathcal{H})}\right].$$&lt;p&gt;
Equivalently, this means that $f=\frac{dQ}{dP}$, the Radon-Nikodym derivative.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!tip] Proposition 2
$Q_{\alpha}$ is a probability measure.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!proof]
We show that $E_{P}[\mathbb{1}_{A}/P(A\mid\mathcal{H})] =E_{P}[1/P(A\mid\mathcal{H})E_{P}[\mathbb{1}_{A}]]=1$, and the second term is likewise $1-\alpha$. Thus $Q_{\alpha}(\Omega)=\alpha+1-\alpha=1$.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[!tip] Proposition 3
$Q_{\alpha}$ is equivalent to $P$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!proof]
$(\Rightarrow )$ Let $N\in \mathcal{F}\text{ with }Q_{\alpha}(N)=0$. Both terms of $f$ are nonnegative, so by $0&lt;P(A\mid\mathcal{H})&lt;1$, each of them must be $0$. $P(A\cap  N)=0\land P(A^{c}\cap N)=0\Rightarrow P(N)=0$.
$(\Leftarrow )$  is trivial.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[!tip] Proposition 4
$Q_{\alpha}(A\mid\mathcal{H})=Q_{\alpha}(A)$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!proof]
First:
&lt;/p&gt;
$$Q_{\alpha}(A)=\alpha E\left[\frac{\mathbb{1}_{A}}{P(A\mid\mathcal{H})}\right]=\alpha E\left[ \frac{1}{P(A\mid\mathcal{H})}E[\mathbb{1}_{A}\mid\mathcal{H}] \right]=\alpha.$$&lt;p&gt;
Second, let $B\in\mathcal{H}$.
Observe that $E_{P}\left[\alpha \frac{\mathbb{1}_{A\cap B}}{P(A\mid\mathcal{H})}\right] = \alpha E\left[\frac{\mathbb{1}_{B}}{P(A\mid\mathcal{H})}E[P(A\mid\mathcal{H})] \right]=\alpha P(B)$ by law of total expectation; and the second term similarly evaluates to $(1-\alpha)P(B)$. Thus $Q_{\alpha}(B)=P(B)$.
Third, $Q_{\alpha}(A\cap B)=\alpha P(B)$, since the first term evaluates as above, and the second is $0$ since $A^{c}\cap A\cap B=\varnothing$.
Finally, $Q(A\mid B)= Q(A\cap B)/Q(B) =\alpha P(B)/P(B)=\alpha$ by definition.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;This construction gives us the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Q(A)=Q(A\mid B)=\alpha\in(0,1)\;\forall B\in\mathcal{H}$, so $A$ is independent of $\mathcal{H}$ under $Q$;&lt;/li&gt;
&lt;li&gt;$Q(B)=P(B)\;\forall B\in\mathcal{H}$, so all events in $\mathcal{H}$ are invariant under this scaling;&lt;/li&gt;
&lt;li&gt;All events that happen a.s. or a.s. never are maintained (equality of measures).&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>HackMIT_2025 Chess</title>
        <link>//localhost:1313/post/hackmit_chess/</link>
        <pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/post/hackmit_chess/</guid>
        <description>&lt;h3 id=&#34;intro&#34;&gt;Intro:
&lt;/h3&gt;&lt;p&gt;This is a write-up of my solution to the Chess challenge from the HackMIT_2025 entry CTF contest. This challenge was the highlight of the contest for me, but not by design. The actual intended solution was fairly unremarkable and dull. What made this fun, however, was the fact that it did not work. I&amp;rsquo;m serious, the challenge was broken. It took them about a week to review the complaints and drop a patch.&lt;/p&gt;
&lt;p&gt;The reason the organizers took so long to double-check this challenge and find the bug was because they saw that one person has solved it &amp;ndash; me, as it happens. The authors designed the solution to be a long clue chase that step by step led the players to decrypt the flag in the last part of the message, but an error made somewhere in the middle of the process made that flag unreachable. So, instead of following a trail of hints that led nowhere, I ended up cracking the encryption scheme itself and reconstructing the flag byte by byte. This solve won me first blood on a broken challenge, secured the leaderboard, and even got me a cheating accusation, which may have been worth more.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;//localhost:1313/post/hackmit_chess/chess_desc.png&#34;
	width=&#34;1032&#34;
	height=&#34;850&#34;
	srcset=&#34;//localhost:1313/post/hackmit_chess/chess_desc_hu_1b3a90f033536a2b.png 480w, //localhost:1313/post/hackmit_chess/chess_desc_hu_4812c656d10ebd0.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Description&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;121&#34;
		data-flex-basis=&#34;291px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution:
&lt;/h3&gt;&lt;p&gt;In this challenge, we are given a ciphertext, as well as a chessboard .png in mate position that contains a hidden chess game PGN. We will not be using that; rather, we observe that the last 64 bytes of the 383-byte text are unique and determined by the userID&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. It is assumed that the userID is hashed to produce the flag, with the bytes of the output being i.i.d. and uniform. We create 30 GitHub accounts, log in through them, and collect the associated ciphertexts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;//localhost:1313/post/hackmit_chess/chess_email.png&#34;
	width=&#34;764&#34;
	height=&#34;376&#34;
	srcset=&#34;//localhost:1313/post/hackmit_chess/chess_email_hu_3d394db5111a333f.png 480w, //localhost:1313/post/hackmit_chess/chess_email_hu_e766b721e0a7b9fa.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;487px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Since only the last 64 bytes ever change, we predict it is a 1-to-1 cipher that individually maps each byte of the plaintext to a byte of the ciphertext.
Then $\exists K$ with $pt_{i}[j]\oplus K[j] =ct_{i}[j]$ for all byte indices $j \in [0,63]$ for all userIDs $i\in [0,30]$.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;
The challenge is reduced to solving a many-time pad.&lt;/p&gt;
&lt;p&gt;We observe further that for any fixed $j\in[63]$, for all $i_{1},i_{2}\in [30]$,
&lt;/p&gt;
$$ct_{i_{1}}[j] \oplus ct_{i_{2}}[j]=pt_{i_{1}}[j]\oplus pt_{i_{2}}[j]\in [\text{0x00, 0x0f}] \cup [\text{0x50,0x5f}].$$&lt;p&gt;This is the distribution of XORed bytes:
&lt;img src=&#34;//localhost:1313/post/hackmit_chess/chess_xors1.png&#34;
	width=&#34;765&#34;
	height=&#34;363&#34;
	srcset=&#34;//localhost:1313/post/hackmit_chess/chess_xors1_hu_fdc34add29963334.png 480w, //localhost:1313/post/hackmit_chess/chess_xors1_hu_4c45a0e0590bc902.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;505px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Let $H=[\text{0x00, 0x0f}]\cup [\text{0x50, 0x5f}]$ be a subgroup of $([\text{0x00, 0xff}],\oplus)$. Then for each fixed index $j$, $pt_{i}[j] \in g_{j}H$, the coset associated with that index formed by &amp;ldquo;shifting&amp;rdquo; $H$.
This limits the set of values from which $pt_{i}[j]$ bytes are drawn to 32. Next we identify the specific set, and show that it is the same for all $j$, and in fact smaller than the full coset.&lt;/p&gt;
&lt;p&gt;In other challenges, the flag has length of 32 bytes, or 64 hex digits. We claim that in this chall, the generated flag is viewed as a string directly, and then encoded into bytes. As a result, each byte is the encoding of a hex digit, of which there are only 16. Consider the distribution of pairwise XORs of the byte encodings of hex digits:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;//localhost:1313/post/hackmit_chess/chess_xors2.png&#34;
	width=&#34;765&#34;
	height=&#34;348&#34;
	srcset=&#34;//localhost:1313/post/hackmit_chess/chess_xors2_hu_251bc5c4fa4982f4.png 480w, //localhost:1313/post/hackmit_chess/chess_xors2_hu_a3c9f8e1c1577d50.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;219&#34;
		data-flex-basis=&#34;527px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;This empirically shows that for all $j$, the set of potential bytes is the encodings of the hex digits. Let this set be called $S$.&lt;/p&gt;
&lt;p&gt;For the last step, we construct a XOR-SAT problem, finding valid values of $K$ under the constraints:
&lt;/p&gt;
$$K[j]\in \bigcap^{n}_{i}\{ct_{i}[j]\oplus s|s\in S\}.$$&lt;p&gt;
As more non-repeating ciphertexts are given, the intersection tightens. A solver like z3 is able to efficiently find valid keys, but how much data is required for strong constraints? We can estimate the number of ciphertexts needed to converge to a manually searchable keyspace.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;I will drop indices to consider a single a.b.f byte of the key $k$.
Let:
&lt;/p&gt;
$$
\begin{align*}
&amp;A_{n}=\bigcap_{i}^{n}\{ct_{i}\oplus s | s \in S\} \\
&amp;B_{n}=\{a\oplus k | a\in A_{n}\}=\bigcap_{i}^{n}\{ pt_{i}\oplus s|s\in S \}\\
\end{align*}
$$&lt;p&gt;
Observe that, given the solvable key exists, $0\in B_{n}$, since $pt_{i}\in S$. We will have solved the problem when only $0$ remains in $B_{n}$, and all the other values have been eliminated. Define the random variable that gives us the smallest satisfying $n$:
&lt;/p&gt;
$$X=\inf\{n|B_{n}=\{0\} \}$$&lt;p&gt;We will estimate $P(X&gt;n)$, the tail probability. Consider a fixed $y$. What is $P(y\in B_{n})$, the probability that $y$ is not eliminated in $n$ ciphertexts? Since we assume the bytes of the plaintexts to be independent, then $P(y\in B_{n})=P(y\in B_{1})^{n}$. Observe following about the event $\{y\in B_{1} \}$:
&lt;/p&gt;
$$
\{y\in B_{1}\} \iff \{\exists s \in S, pt=y\oplus s \} \iff \{ pt\in y\oplus S= \{y\oplus s |s\in S\}  \}.
$$&lt;p&gt;
Recall that $pt\in S$. Then: $P(pt\in y\oplus S)= \frac{|S\cap y\oplus S|}{|S|}$. For convenience, define $f_{\oplus}$, the &amp;ldquo;autocorrelation&amp;rdquo; function between $S$ and the XOR-shifted $S$ by $f_{\oplus}(y)= |S\cap y\oplus S|$.
Then for all $y$,
&lt;/p&gt;
$$P(y\in B_{1})= \frac{{f_{\oplus}(y)}}{|S|}, \text{ and } P(y\in B_{n})= \left( \frac{{f_{\oplus}(y)}}{|S|} \right) ^{n}.$$&lt;p&gt;Finally:
&lt;/p&gt;
$$
\begin{align*}
P(X&gt;n) &amp;= P \left( \bigcup_{y\neq 0} \{y\in B_{n}\} \right) \quad \text{some y is not eliminated}\\
&amp;\leq \sum\limits_{y\neq 0} P(y\in B_{n}) \qquad \text{  by sigma-additivity}\\
&amp;= \sum\limits_{y\neq0} \left( \frac{{f_{\oplus}(y)}}{|S|} \right)^{n} \\&amp;= \frac{255}{|S|^{n}} \sum\limits_{y\neq0}f_{\oplus}(y)^{n}
\end{align*}
$$&lt;p&gt;
The outcome depends on the structure of $f_\oplus$, which is in turn determined by the contents of S. A fun consequence of $S\subset gH$ is that $f_{\oplus}(y)&gt;0 \iff y\in H$. The proof is an exercise for the reader. &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; In practice, this means the distribution of $f_{\oplus}$ is heavily skewed towards 0, giving us a relatively small value in the sum.&lt;/p&gt;
$$
\begin{align*}
E[X]&amp;=\sum\limits_{n=0}^{\infty} P(X&gt;n)\\
&amp;= P(X&gt;0)+P(X&gt;1) + \sum\limits_{n=2}^{\infty} P(X&gt;n)\\
&amp;\leq 2+ \sum\limits_{n=2}^{\infty} \sum\limits_{y\in H\setminus0} (f_\oplus(y)|S|^{-1})^{n}\\
&amp;= 2+\sum\limits_{y\in H\setminus0} \sum\limits_{n=2}^{\infty} (f_\oplus(y)|S|^{-1})^{n}\\
&amp;= 2+\sum\limits_{y\in H\setminus0} \frac{(f_\oplus(y)|S|^{-1})^{2}}{1-f_\oplus(y)|S|^{-1}} \qquad \text{geometric series}\\
&amp;=2+ \frac{31}{|S|^{2}} \sum\limits_{y\in H\setminus 0} \frac{f_{\oplus}(y)^{2}}{ 1-f_\oplus(y)|S|^{-1}} \\ 
&amp;\approx 44.57 \quad \text{ciphertexts needed to reduce keyspace to 1.}
\end{align*}
$$&lt;p&gt;I used 30 ciphertexts to solve for the key, with $P(X&gt;30)\leq 0.0387$. This is the probability an a.b.f. key byte fails to converge to a single value. Since all bytes are independent, the number of bytes that fail to converge out of 64 follows a binomial distribution $\text{Bin}(64, 0.0387)$, and has $E[\cdot]\approx 2.48$. In my actual case, 4 bytes failed to converge (with 2 candidate bytes each), giving 16 possible keys.&lt;/p&gt;
&lt;p&gt;From there it is trivial to compute the flag by taking XOR with the key.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;The hint for this is in the webpage&amp;rsquo;s ciphertext retrieval endpoint.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;This makes the assumption that the encryption scheme can be expressed as a XOR against some consistent implicit key. It would not be true in, for example, a permutation cipher. I suspected the moves in the chess game were used as the encryption key, which would satisfy this. The next observation effectively confirmed it, as the common key was being cancelled out in the XOR.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;I did the math post factum. At the time of solving, I simply generated new cts until it was few enough keys to brute force, but I was curious what the theoretical expected num of cts was, so I went back to solve for it afterwards.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;How it feels to write this üî•‚úçÔ∏è&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>About Me</title>
        <link>//localhost:1313/page/about/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/page/about/</guid>
        <description>&lt;p&gt;Welcome to my page. I will get this place in order one day.&lt;/p&gt;
&lt;p&gt;My name is Anton and I&amp;rsquo;m an undergrad studying math and compsci at Arizona State university. I focus broadly in probability theory but I like and do all sorts of things.&lt;/p&gt;
&lt;p&gt;I occasionally play CTF(Capture the Flag) competitions with our local uni team, CTF Academy, as well as with Shellphish.&lt;/p&gt;
&lt;p&gt;I speak Russian and English and at various points have also studied Spanish and Mandarin.&lt;/p&gt;
&lt;h3 id=&#34;heres-some-stuff-i-like-in-no-particular-order&#34;&gt;Here&amp;rsquo;s some stuff I like, in no particular order:
&lt;/h3&gt;&lt;p&gt;‚ú∂ Stochastic processes &amp;amp; models, statistical algos, information theory;&lt;/p&gt;
&lt;p&gt;‚ú∂ Diffusion models, reinforcement learning;&lt;/p&gt;
&lt;p&gt;‚ú∂ Random, approximation, and reconstruction algorithms;&lt;/p&gt;
&lt;p&gt;‚ú∂ Graph theory;&lt;/p&gt;
&lt;p&gt;‚ú∂ Cryptography &amp;amp; cryptanalysis;&lt;/p&gt;
&lt;p&gt;‚ú∂ Computational biology;&lt;/p&gt;
&lt;h3 id=&#34;what-im-working-on-right-now&#34;&gt;What I&amp;rsquo;m working on right now:
&lt;/h3&gt;&lt;p&gt;‚ú´ &lt;strong&gt;Minimum entropy coupling algorithms&lt;/strong&gt;: this implements some techniques from a series of papers on algorithmic information theory; namely, how to construct a coupling between two probability distribution that has the smallest possible entropy, i.e., &amp;ldquo;most ordered&amp;rdquo;. In general, the problem is NP-hard, but there are approximation algorithms for different kinds of probability distributions. One interesting use case is in steganography, where you can construct a coupling with an LLM&amp;rsquo;s conditional token distribution and generate covertext that contains hidden data while maintaining provable perfect secrecy.&lt;/p&gt;
&lt;p&gt;‚ú´ &lt;strong&gt;Random process algorithms on graphs&lt;/strong&gt;: this is a research project for my class on stochastic modelling! I&amp;rsquo;ve been spending some time investigating graph algorithms that can be modelled as stochastic processes. Some interesting topics in this area are on random approximations to problems like hypergraph packing, vertex cover search, and graphon estimation.&lt;/p&gt;
&lt;p&gt;‚ú´ &lt;strong&gt;DeepMapDB&lt;/strong&gt;: this has been a long-running on-and-off r&amp;amp;d project on the following question: How much data can you fit in a neural network? Put more specifically, given a dictionary-format database, what is the smallest NN that can &amp;ldquo;memorize it&amp;rdquo; under fixed train time constraints and up to near-perfect accuracy? This task has applications in scenarios in plain data compression, DMBS with high-volume querying requirements, and database deployment on space and memory-constrained hardware.&lt;/p&gt;
&lt;p&gt;‚ú´ &lt;strong&gt;crypto.college&lt;/strong&gt;: my university uses pwn.college, a platform for hosting CTF-style challenges to teach its cybersec classes. Check it out, it&amp;rsquo;s brilliant. Though security such as low-level exploitation is not really my forte, I have some knowledge of cryptography from playing CTFs and personal research. I&amp;rsquo;m currently working with a team on expanding pwn.college to include several modules of cryptography material &amp;amp; challenges, which will be hopefully used one day to teach classes at ASU!&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>//localhost:1313/page/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/page/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
