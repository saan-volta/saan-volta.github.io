[{"content":"0. What is this? This is intended to be an in-detail walkthrough of how measure-theoretic probability works under the hood. When I took my first course on this topic, I found that there was not a lot of material that actually connected the measure theory \u0026ldquo;machine code\u0026rdquo; to probability problems. I\u0026rsquo;ve put together some explanations and examples that would\u0026rsquo;ve been useful to me when I first learned about it. This assumes knowledge of (very) basic measure theory \u0026amp; probability.\n1. Here\u0026rsquo;s what you have to know [!info] Definition: Measure space $(\\Omega,\\mathcal{F},\\mu)$ is a measure (probability) space, where:\n$\\Omega$ is a set (of outcomes); $\\mathcal{F}$ is a $\\sigma$-algebra on $\\Omega$, consisting of subsets of $\\Omega$ and satisfying the following: $\\varnothing,\\Omega\\in\\mathcal{F}$; $A\\in\\mathcal{F}\\Rightarrow A^{c}\\in\\mathcal{F}$ (stable under complement); $\\set{A_{n}}\\in\\mathcal{F}\\Rightarrow \\bigcup_{n}A_{n}\\in\\mathcal{F}$ (stable under countable union); In probability, this is the event space. $\\mu:\\mathcal{F}\\rightarrow [0,\\infty]$ is the measure function $\\mu$ satisfies $\\sigma$-additivity, i.e.: $\\mu(\\bigsqcup_{n}A_{n})=\\sum_{n}\\mu(A_{n})$; $\\mu(\\varnothing)=0$; In probability, the measure is written as $P$, has codomain $[0,1]$, and $P(\\Omega)=1$. Comments:\n$\\mathcal{F}$ is stable by (countable) intersection; $P(\\varnothing)=0$ and $P(\\Omega)=1$ mean that something must happen; The trivial (coarsest) $\\sigma$-algebra over $\\Omega$ is $\\set{\\Omega,\\varnothing}$, and the fullest (finest) is $\\mathcal{P}(\\Omega)$ (if $\\Omega$ is countable). [!info] Definition: Random variable A (real-valued) random variable in a probability space $(\\Omega,\\mathcal{F},P)$ is a function $X:\\Omega\\rightarrow \\mathbb{R}$ that is measurable, i.e., satisfies: $$\\forall B\\in\\mathcal{B}(\\mathbb{R})\\quad X^{-1}(B)\\in\\mathcal{F}.$$ where $\\mathcal{B}(\\mathbb{R})$ is the Borel $\\sigma$-algebra of the real line.\nIf you have no measure theory background, $\\mathcal{B}(\\mathbb{R})$ might be hard to conceptualize. It is not the powerset of $\\mathbb{R}$, but it may be useful to think of it that way. The subsets of $\\mathbb{R}$ which do not fall in it require nontrivial and fairly contrived constructions that will never come up in a probability application.\nHere are some useful identities/definitions about expectation:\n[!note]\n$E[X\\mid Y]=E[X\\mid\\sigma(Y)]$ (this is just a shorthand) $E[X]=E[E[X\\mid \\mathcal{A}]]$ (law of total expectation) $E[\\mathbb{1}_{A}X]=E[\\mathbb{1}_{A}E[X\\mid\\mathcal{A}]]\\quad \\forall A\\in\\mathcal{A}\\quad$ (def. of conditional expectation) $E[aX+bY]=aE[X]+bE[Y]$ (linearity of expectation) $E[X\\mid \\mathcal{A}]=X$ when $X$ is $\\mathcal{A}$-measurable and $E[X]$ when $X,\\mathcal{A}$ indep. $E[XY\\mid\\mathcal{A}]=XE[Y\\mid\\mathcal{A}]$ when $X$ is $\\mathcal{A}$-measurable 2. The conditioner Let $(\\Omega,\\mathcal{F},P)$ be a probability space and let $A\\in\\mathcal{F}$ be an event. In this segment, we will examine what it means to condition $P(A\\mid \\cdot)$ on a $\\sigma$-algebra.\n[!note] Note I want to emphasize that when we write $P(A)$, what we mean mathematically is $E[\\mathbb{1}_{A}]=\\int_{\\Omega}\\mathbb{1}_{A}dP$, and more explicitly $\\int_{\\Omega}\\mathbb{1}_{A}(\\omega)dP(\\omega)$, where $\\mathbb{1}_{A}(\\cdot)$ is in fact a random variable.\nLet\u0026rsquo;s look at the easiest case: suppose $B\\in\\mathcal{F}$ is another event, then $P(A\\mid B)$ is just a number. Here, we simply condition on the realization that $B$ occurs.\nWhat if we don\u0026rsquo;t know whether it does? In such a case, we would condition on $\\sigma(B)$, the $\\sigma$-algebra generated by this event. The interesting thing is that $P(A\\mid\\sigma(B))$ is in fact a random variable (a measurable function of $\\omega$). Intuitively, $\\sigma(B)$ gives us some information about $A$, but the information is random and dependent on where $\\omega$ lands. Since $\\sigma(B)=\\set{\\varnothing,B,B^{c},\\Omega}$ has two nontrivial possibilities, the value of $P(A\\mid\\sigma(B))(\\omega)$ depends on whether $\\omega\\in B$ or $\\in B^{c}$: $$P(A\\mid\\sigma(B))=\\mathbb{1}_{B}P(A|B)+\\mathbb{1}_{B^{c}}P(A|B^{c}).$$ This works out nicely because $\\set{B,B^{c}}$ is a partition of the outcome space. We can easily extend this \u0026ndash; let $\\set{B_{n}}$ be a partition, i.e., a collection of disjoint subsets of $\\Omega$ that add up to the whole set. Then: $$P(A\\mid\\sigma\\set{B_{n}})=\\sum\\limits_{n}\\mathbb{1}_{B_{n}}P(A\\mid B_{n}).$$ We extend this further to compute the expectation of a general random variable: $$E[X\\mid\\sigma\\set{B_{n}}]=\\sum\\limits_{n}\\mathbb{1}_{B_{n}} E[X\\mid B_{n}]=\\sum\\limits_{n}\\mathbb{1}_{B_{n}} \\frac{1}{P(B_{n})}\\int_{B_{n}}XdP.$$3. Measurability Let $\\mathcal{H}\\subset \\mathcal{F}$ and $A\\in \\mathcal{F}$. The definition of measurable functions tells us that $A$ being $\\mathcal{H}$-measurable means that $\\forall B\\in\\mathcal{B}(\\mathbb{R}), \\mathbb{1}_{A}^{-1}(B)\\in \\mathcal{H}$. There are four cases, dictated by whether $0$ and $1$ are in $B$. If they both are or aren\u0026rsquo;t, we get that the preimage is respectively $\\Omega$ or $\\varnothing$, which is trivially true as $\\mathcal{H}$ contains them by definition. If only $1\\in B$, then the preimage is $\\set{\\omega\\in A}$, and if $0\\in B$ it is $\\set{\\omega\\notin A}$. Note that it is sufficient for one of these to be in $\\mathcal{H}$, since the other will be contained automatically as complement. Since $\\sigma(A)\\subseteq \\mathcal{H}$, $A$ is thus completely determinable. So we have: $$A\\in\\mathcal{H}\\Leftrightarrow \\mathbb{1}_{A}(\\cdot)\\; \\mathcal{H}\\text{-measurable }\\Leftrightarrow P(A\\mid\\mathcal{H})=E[\\mathbb{1}_{A}\\mid\\mathcal{H}]=\\mathbb{1}_{A}\\in \\set{0,1}\\text{ a.s. }$$Let\u0026rsquo;s now consider that $0","date":"2025-12-30T00:00:00Z","permalink":"//localhost:1313/post/how_to_measure/","title":"How to Measure"},{"content":"Intro: This is a write-up of my solution to the Chess challenge from the HackMIT_2025 entry CTF contest. This challenge was the highlight of the contest for me, but not by design. The actual intended solution was fairly unremarkable and dull. What made this fun, however, was the fact that it did not work. I\u0026rsquo;m serious, the challenge was broken. It took them about a week to review the complaints and drop a patch.\nThe reason the organizers took so long to double-check this challenge and find the bug was because they saw that one person has solved it \u0026ndash; me, as it happens. The authors designed the solution to be a long clue chase that step by step led the players to decrypt the flag in the last part of the message, but an error made somewhere in the middle of the process made that flag unreachable. So, instead of following a trail of hints that led nowhere, I ended up cracking the encryption scheme itself and reconstructing the flag byte by byte. This solve won me first blood on a broken challenge, secured the leaderboard, and even got me a cheating accusation, which may have been worth more.\nDescription Solution: In this challenge, we are given a ciphertext, as well as a chessboard .png in mate position that contains a hidden chess game PGN. We will not be using that; rather, we observe that the last 64 bytes of the 383-byte text are unique and determined by the userID1. It is assumed that the userID is hashed to produce the flag, with the bytes of the output being i.i.d. and uniform. We create 30 GitHub accounts, log in through them, and collect the associated ciphertexts.\nSince only the last 64 bytes ever change, we predict it is a 1-to-1 cipher that individually maps each byte of the plaintext to a byte of the ciphertext. Then $\\exists K$ with $pt_{i}[j]\\oplus K[j] =ct_{i}[j]$ for all byte indices $j \\in [0,63]$ for all userIDs $i\\in [0,30]$.2 The challenge is reduced to solving a many-time pad.\nWe observe further that for any fixed $j\\in[63]$, for all $i_{1},i_{2}\\in [30]$, $$ct_{i_{1}}[j] \\oplus ct_{i_{2}}[j]=pt_{i_{1}}[j]\\oplus pt_{i_{2}}[j]\\in [\\text{0x00, 0x0f}] \\cup [\\text{0x50,0x5f}].$$This is the distribution of XORed bytes: Let $H=[\\text{0x00, 0x0f}]\\cup [\\text{0x50, 0x5f}]$ be a subgroup of $([\\text{0x00, 0xff}],\\oplus)$. Then for each fixed index $j$, $pt_{i}[j] \\in g_{j}H$, the coset associated with that index formed by \u0026ldquo;shifting\u0026rdquo; $H$. This limits the set of values from which $pt_{i}[j]$ bytes are drawn to 32. Next we identify the specific set, and show that it is the same for all $j$, and in fact smaller than the full coset.\nIn other challenges, the flag has length of 32 bytes, or 64 hex digits. We claim that in this chall, the generated flag is viewed as a string directly, and then encoded into bytes. As a result, each byte is the encoding of a hex digit, of which there are only 16. Consider the distribution of pairwise XORs of the byte encodings of hex digits:\nThis empirically shows that for all $j$, the set of potential bytes is the encodings of the hex digits. Let this set be called $S$.\nFor the last step, we construct a XOR-SAT problem, finding valid values of $K$ under the constraints: $$K[j]\\in \\bigcap^{n}_{i}\\{ct_{i}[j]\\oplus s|s\\in S\\}.$$ As more non-repeating ciphertexts are given, the intersection tightens. A solver like z3 is able to efficiently find valid keys, but how much data is required for strong constraints? We can estimate the number of ciphertexts needed to converge to a manually searchable keyspace.3\nI will drop indices to consider a single a.b.f byte of the key $k$. Let: $$ \\begin{align*} \u0026A_{n}=\\bigcap_{i}^{n}\\{ct_{i}\\oplus s | s \\in S\\} \\\\ \u0026B_{n}=\\{a\\oplus k | a\\in A_{n}\\}=\\bigcap_{i}^{n}\\{ pt_{i}\\oplus s|s\\in S \\}\\\\ \\end{align*} $$ Observe that, given the solvable key exists, $0\\in B_{n}$, since $pt_{i}\\in S$. We will have solved the problem when only $0$ remains in $B_{n}$, and all the other values have been eliminated. Define the random variable that gives us the smallest satisfying $n$: $$X=\\inf\\{n|B_{n}=\\{0\\} \\}$$We will estimate $P(X\u003en)$, the tail probability. Consider a fixed $y$. What is $P(y\\in B_{n})$, the probability that $y$ is not eliminated in $n$ ciphertexts? Since we assume the bytes of the plaintexts to be independent, then $P(y\\in B_{n})=P(y\\in B_{1})^{n}$. Observe following about the event $\\{y\\in B_{1} \\}$: $$ \\{y\\in B_{1}\\} \\iff \\{\\exists s \\in S, pt=y\\oplus s \\} \\iff \\{ pt\\in y\\oplus S= \\{y\\oplus s |s\\in S\\} \\}. $$ Recall that $pt\\in S$. Then: $P(pt\\in y\\oplus S)= \\frac{|S\\cap y\\oplus S|}{|S|}$. For convenience, define $f_{\\oplus}$, the \u0026ldquo;autocorrelation\u0026rdquo; function between $S$ and the XOR-shifted $S$ by $f_{\\oplus}(y)= |S\\cap y\\oplus S|$. Then for all $y$, $$P(y\\in B_{1})= \\frac{{f_{\\oplus}(y)}}{|S|}, \\text{ and } P(y\\in B_{n})= \\left( \\frac{{f_{\\oplus}(y)}}{|S|} \\right) ^{n}.$$Finally: $$ \\begin{align*} P(X\u003en) \u0026= P \\left( \\bigcup_{y\\neq 0} \\{y\\in B_{n}\\} \\right) \\quad \\text{some y is not eliminated}\\\\ \u0026\\leq \\sum\\limits_{y\\neq 0} P(y\\in B_{n}) \\qquad \\text{ by sigma-additivity}\\\\ \u0026= \\sum\\limits_{y\\neq0} \\left( \\frac{{f_{\\oplus}(y)}}{|S|} \\right)^{n} \\\\\u0026= \\frac{255}{|S|^{n}} \\sum\\limits_{y\\neq0}f_{\\oplus}(y)^{n} \\end{align*} $$ The outcome depends on the structure of $f_\\oplus$, which is in turn determined by the contents of S. A fun consequence of $S\\subset gH$ is that $f_{\\oplus}(y)\u003e0 \\iff y\\in H$. The proof is an exercise for the reader. 4 In practice, this means the distribution of $f_{\\oplus}$ is heavily skewed towards 0, giving us a relatively small value in the sum.\n$$ \\begin{align*} E[X]\u0026=\\sum\\limits_{n=0}^{\\infty} P(X\u003en)\\\\ \u0026= P(X\u003e0)+P(X\u003e1) + \\sum\\limits_{n=2}^{\\infty} P(X\u003en)\\\\ \u0026\\leq 2+ \\sum\\limits_{n=2}^{\\infty} \\sum\\limits_{y\\in H\\setminus0} (f_\\oplus(y)|S|^{-1})^{n}\\\\ \u0026= 2+\\sum\\limits_{y\\in H\\setminus0} \\sum\\limits_{n=2}^{\\infty} (f_\\oplus(y)|S|^{-1})^{n}\\\\ \u0026= 2+\\sum\\limits_{y\\in H\\setminus0} \\frac{(f_\\oplus(y)|S|^{-1})^{2}}{1-f_\\oplus(y)|S|^{-1}} \\qquad \\text{geometric series}\\\\ \u0026=2+ \\frac{31}{|S|^{2}} \\sum\\limits_{y\\in H\\setminus 0} \\frac{f_{\\oplus}(y)^{2}}{ 1-f_\\oplus(y)|S|^{-1}} \\\\ \u0026\\approx 44.57 \\quad \\text{ciphertexts needed to reduce keyspace to 1.} \\end{align*} $$I used 30 ciphertexts to solve for the key, with $P(X\u003e30)\\leq 0.0387$. This is the probability an a.b.f. key byte fails to converge to a single value. Since all bytes are independent, the number of bytes that fail to converge out of 64 follows a binomial distribution $\\text{Bin}(64, 0.0387)$, and has $E[\\cdot]\\approx 2.48$. In my actual case, 4 bytes failed to converge (with 2 candidate bytes each), giving 16 possible keys.\nFrom there it is trivial to compute the flag by taking XOR with the key.\nThe hint for this is in the webpage\u0026rsquo;s ciphertext retrieval endpoint.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis makes the assumption that the encryption scheme can be expressed as a XOR against some consistent implicit key. It would not be true in, for example, a permutation cipher. I suspected the moves in the chess game were used as the encryption key, which would satisfy this. The next observation effectively confirmed it, as the common key was being cancelled out in the XOR.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI did the math post factum. At the time of solving, I simply generated new cts until it was few enough keys to brute force, but I was curious what the theoretical expected num of cts was, so I went back to solve for it afterwards.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow it feels to write this üî•‚úçÔ∏è\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-09-30T00:00:00Z","permalink":"//localhost:1313/post/hackmit_chess/","title":"HackMIT_2025 Chess"}]